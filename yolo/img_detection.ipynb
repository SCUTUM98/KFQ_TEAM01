{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING ⚠️ 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\n",
      "Note this warning may be related to loading older models. You can update your model to current structure with:\n",
      "    import torch\n",
      "    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n",
      "    torch.save(ckpt, \"updated-model.pt\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from yolov5.models.experimental import attempt_load\n",
    "\n",
    "from convert_data import collect_data\n",
    "from class_mapping import yolo_class_mapping, fire_class_mapping, animals_class_mapping\n",
    "from detection import category\n",
    "from perform_object import perform_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opencv version:  4.8.0\n"
     ]
    }
   ],
   "source": [
    "print('opencv version: ', cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5m summary: 369 layers, 21190557 parameters, 0 gradients\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 369 layers, 20875359 parameters, 0 gradients, 48.2 GFLOPs\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 369 layers, 20887482 parameters, 0 gradients, 48.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Time, Class_Name, Confidence, X1, Y1, X2, Y2, action_detection, action_category]\n",
      "Index: []\n",
      "                  Time Class_Name       Confidence    X1   Y1    X2   Y2  \\\n",
      "0  2023-08-17 13:42:17        car  tensor(0.75091)   769  656  1170  933   \n",
      "1  2023-08-17 13:42:17        car  tensor(0.71840)  1149  692  1497  867   \n",
      "\n",
      "   action_detection action_category  \n",
      "0                 0             NaN  \n",
      "1                 0             NaN  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m results_df \u001b[39m=\u001b[39m perform_object(image_tensor, \u001b[39m0.65\u001b[39m, yolo_model, frame, yolo_class_mapping, results_df, captured_time)\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(results_df)\n\u001b[0;32m---> 34\u001b[0m results_df \u001b[39m=\u001b[39m perform_object(image_tensor, \u001b[39m0.65\u001b[39;49m, yolo_model, frame, fire_class_mapping, results_df, captured_time)\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(results_df) \n\u001b[1;32m     36\u001b[0m results_df \u001b[39m=\u001b[39m perform_object(image_tensor, \u001b[39m0.65\u001b[39m, yolo_model, frame, animals_class_mapping, results_df, captured_time)\n",
      "File \u001b[0;32m~/Desktop/kfq/KFQ_TEAM01/yolo/perform_object.py:7\u001b[0m, in \u001b[0;36mperform_object\u001b[0;34m(image_tensor, conf_thres, model, frame, class_mapping, results_df, captured_time)\u001b[0m\n\u001b[1;32m      5\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(image_tensor)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m results \u001b[39m=\u001b[39m non_max_suppression(results, conf_thres\u001b[39m=\u001b[39mconf_thres)\n\u001b[0;32m----> 7\u001b[0m results_df \u001b[39m=\u001b[39m collect_data(results, frame, class_mapping, results_df, captured_time)\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m results_df\n",
      "File \u001b[0;32m~/Desktop/kfq/KFQ_TEAM01/yolo/convert_data.py:16\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(results, frame, mapping, results_df, captured_time)\u001b[0m\n\u001b[1;32m     13\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(x2 \u001b[39m*\u001b[39m (original_width \u001b[39m/\u001b[39m input_size[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     14\u001b[0m y2 \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(y2 \u001b[39m*\u001b[39m (original_height \u001b[39m/\u001b[39m input_size[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 16\u001b[0m class_name \u001b[39m=\u001b[39m mapping[\u001b[39mint\u001b[39;49m(\u001b[39mcls\u001b[39;49m)]\n\u001b[1;32m     17\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClass: \u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m, Confidence: \u001b[39m\u001b[39m{\u001b[39;00mconf\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m color \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m)  \u001b[39m# Green color for bounding box\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "camera_name = 'KFQ_StarValley'\n",
    "\n",
    "# Load trained YOLOv5 model weights\n",
    "yolo_weights_path = '/Users/bongeungu/Desktop/kfq/KFQ_TEAM01/yolo/models/yolov5m.pt'\n",
    "device = torch.device('cpu')\n",
    "yolo_model = attempt_load(yolo_weights_path, device=device)\n",
    "\n",
    "fire_weights_path = '/Users/bongeungu/Downloads/fire3.pt'\n",
    "fire_model = attempt_load(fire_weights_path, device=device)\n",
    "\n",
    "animals_weights_path = '/Users/bongeungu/Desktop/kfq/KFQ_TEAM01/yolo/models/animals.pt'\n",
    "animals_model = attempt_load(animals_weights_path, device=device)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "columns = ['Time', 'Class_Name', 'Confidence', 'X1', 'Y1', 'X2', 'Y2', 'action_detection', 'action_category']\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Read frame from the webcam\n",
    "    captured_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") # 상황 발생 시간 기록\n",
    "    \n",
    "    # Preprocess the frame\n",
    "    image = Image.fromarray(frame)\n",
    "    image = image.resize((640, 640))  # Resize the image to the model's input size\n",
    "    image_tensor = torch.from_numpy(np.array(image)).float().permute(2, 0, 1) / 255.0\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform object detection\n",
    "    results = perform_object(image_tensor, 0.65, yolo_model)\n",
    "    results_df = collect_data(results, frame, yolo_class_mapping, results_df, captured_time)\n",
    "    print(results_df)\n",
    "    \n",
    "    results = perform_object(image_tensor, 0.65, fire_model)\n",
    "    results_df = collect_data(results, frame, fire_class_mapping, results_df, captured_time)\n",
    "    print(results_df)\n",
    "    \n",
    "    results = perform_object(image_tensor, 0.65, animals_model)\n",
    "    results_df = collect_data(results, frame, animals_class_mapping, results_df, captured_time)\n",
    "    print(results_df)\n",
    "    \n",
    "    results_df = category(results_df)\n",
    "\n",
    "    # Display the frame with bounding boxes and labels\n",
    "    cv2.imshow(camera_name, frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit the loop\n",
    "        break\n",
    "    \n",
    "    time.sleep(2) # 2초 간격으로 사진 캡쳐(30 frames per 1 miniute)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('object_detection_results.csv', index=False)\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
